{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8ZgYovd1D0w1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3YFgaj_cEE2o"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"data/train_merged.csv\")\n",
        "test =  pd.read_csv(\"data/test_merged.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rewde-UEkye"
      },
      "source": [
        "# Base Line training - just encoded data, fill missing and use all column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBwrleKkEK5-",
        "outputId": "5ec5e3ab-d8ea-497c-e744-a357ecc2cd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12014 entries, 0 to 12013\n",
            "Columns: 434 entries, TransactionID to DeviceInfo\n",
            "dtypes: float64(399), int64(4), object(31)\n",
            "memory usage: 39.8+ MB\n"
          ]
        }
      ],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cWlLnQe1G9vH"
      },
      "outputs": [],
      "source": [
        "target = 'isFraud'\n",
        "y = train[target]\n",
        "X = train.drop(columns=[target])\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2B1K8TXOH_z1"
      },
      "outputs": [],
      "source": [
        "numeic_tf = Pipeline (\n",
        "    steps = [\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_tf = Pipeline (\n",
        "    steps = [\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocess = ColumnTransformer (\n",
        "    transformers = [\n",
        "        ('num', numeic_tf, num_cols),\n",
        "        ('cat', categorical_tf, cat_cols),\n",
        "    ] ,\n",
        "    remainder = 'drop'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAJEQN8XIpLw"
      },
      "source": [
        "## Logistic regression , Decision tree , Random Forest, Xg boost, lightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sZi3Hp4fJ3w_"
      },
      "outputs": [],
      "source": [
        "to_dense = FunctionTransformer(\n",
        "    lambda x: x.toarray() if hasattr(x, \"toarray\") else x,\n",
        "    accept_sparse=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qrOjKczGKSRy"
      },
      "outputs": [],
      "source": [
        "pos = y.sum()\n",
        "neg = len(y) - pos\n",
        "scale_pos_weight = float(neg / pos) if pos > 0 else 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KzJFj-gkIpA-"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"logistic\": Pipeline([\n",
        "        (\"prep\", preprocess),\n",
        "        (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
        "    ]),\n",
        "    \"decision_tree\": Pipeline([\n",
        "        (\"prep\", preprocess),\n",
        "        (\"to_dense\", to_dense),\n",
        "        (\"clf\", DecisionTreeClassifier(class_weight=\"balanced\", random_state=42))\n",
        "    ]),\n",
        "    \"random_forest\": Pipeline([\n",
        "        (\"prep\", preprocess),\n",
        "        (\"to_dense\", to_dense),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            n_estimators=300, random_state=42, n_jobs=-1,\n",
        "            class_weight=\"balanced_subsample\"\n",
        "        ))\n",
        "    ]),\n",
        "}\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    models[\"xgboost\"] = Pipeline([\n",
        "        (\"prep\", preprocess),\n",
        "        (\"clf\", XGBClassifier(\n",
        "            n_estimators=500, learning_rate=0.05, max_depth=6,\n",
        "            subsample=0.8, colsample_bytree=0.8, tree_method=\"hist\",\n",
        "            eval_metric=\"logloss\", n_jobs=-1, scale_pos_weight=scale_pos_weight\n",
        "        ))\n",
        "    ])\n",
        "except Exception as e:\n",
        "    print(\" XGBoost Error\", e)\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    models[\"lightgbm\"] = Pipeline([\n",
        "        (\"prep\", preprocess),\n",
        "        (\"clf\", LGBMClassifier(\n",
        "            n_estimators=500, learning_rate=0.05, num_leaves=64,\n",
        "            subsample=0.8, colsample_bytree=0.8, n_jobs=-1,\n",
        "            class_weight=\"balanced\"\n",
        "        ))\n",
        "    ])\n",
        "except Exception as e:\n",
        "    print(\"LightGBM Error\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UHHrTVwIdYa",
        "outputId": "027b0897-6935-4d5c-cb91-99905345578e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic     AUC = 0.6752\n",
            "decision_tree AUC = 0.6906\n",
            "random_forest AUC = 0.8533\n",
            "xgboost      AUC = 0.8718\n",
            "[LightGBM] [Info] Number of positive: 262, number of negative: 9349\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032465 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 14907\n",
            "[LightGBM] [Info] Number of data points in the train set: 9611, number of used features: 495\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lightgbm     AUC = 0.8630\n",
            "\n",
            " Sorting AUC decesnding :\n",
            "- xgboost: 0.8718\n",
            "- lightgbm: 0.8630\n",
            "- random_forest: 0.8533\n",
            "- decision_tree: 0.6906\n",
            "- logistic: 0.6752\n"
          ]
        }
      ],
      "source": [
        "X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "results = {}\n",
        "for name, pipe in models.items():\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    if hasattr(pipe.named_steps[list(pipe.named_steps.keys())[-1]], \"predict_proba\"):\n",
        "        proba = pipe.predict_proba(X_va)[:, 1]\n",
        "    else:\n",
        "        from sklearn.calibration import CalibratedClassifierCV\n",
        "        cal = CalibratedClassifierCV(pipe, cv=3)\n",
        "        cal.fit(X_tr, y_tr)\n",
        "        proba = cal.predict_proba(X_va)[:, 1]\n",
        "    auc = roc_auc_score(y_va, proba)\n",
        "    results[name] = auc\n",
        "    print(f\"{name:12s} AUC = {auc:.4f}\")\n",
        "\n",
        "print(\"\\n Sorting AUC decesnding :\")\n",
        "for k, v in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"- {k}: {v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhance by : feature engineering , feature selection , data preprocessing deely , scaling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyper tunning parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Pz8uUrdbLA1g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
